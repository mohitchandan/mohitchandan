{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f24aff-0d7e-41ca-a021-9895caa8ae7b",
   "metadata": {},
   "source": [
    "<h3>6.\tLet’s carry out fraud detection on a very large transactional dataset – assume that it is 1 TB in size – which is generated by a retail chain with multiple store locations.\n",
    "</h3>\n",
    "In the retail space, an SKU is a unique identifier or code that refers to a particular stock keeping unit. In our context, we shall denote each SKU with an alphabet followed by a 6-digit sequence, for example, B432156\n",
    "\n",
    "Every row in our dataset captures one complete transaction and can be represented as shown via a sequence: k here denotes the number of item types in the transaction. \n",
    "\n",
    "{ Store ID,\n",
    "  Transaction Date,\n",
    "  (Item 1 SKU, Quantity Purchased of Item 1, Unit Cost of Item 1), \n",
    "  (Item 2 SKU, Quantity Purchased of Item 2, Unit Cost of Item 2),\n",
    "      . . . \n",
    "  (Item k SKU, Quantity Purchased of Item k, Unit Cost of Item k) }\n",
    "\n",
    "In a flattened out “relational” format, transaction sequences might appear as shown, with items, their quantities of purchase and unit costs listed in serial order.\n",
    "\n",
    " \n",
    "\n",
    "Assume you have been supplied a function isFraud(transaction sequence) that checks if an input transaction is fraudulent or not, and returns TRUE or FALSE. Skip worrying about the construction of the function isFraud or of its correctness – just assume that it is adequate for our purposes. \n",
    "\n",
    "As the proprietor of the chain, you wish to know which transactions are fraudulent, and just as importantly, what is the total amount of fraud by store.\n",
    "\n",
    "Assume that the data is contained in a tab-separated file AllTransactions.txt. \n",
    "\n",
    "a.\tIn simple words, explain how you would use the Hadoop framework to both represent the problem and then solve it. Elaborate on the dynamics of what you would first do with the data file, what will Hadoop do next, and so on.\n",
    "\n",
    "b.\tWrite down a mapper and reducer in a language of your choice, so long as it is commented properly. What are the inputs to each function? What logic will each function perform? Clearly explain how the final answer is arrived at. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c35400a-4004-4029-ac5f-19e11f06699d",
   "metadata": {},
   "source": [
    "<h3>a. In simple words, explain how you would use the Hadoop framework to both represent the problem and then solve it. Elaborate on the dynamics of what you would first do with the data file, what will Hadoop do next, and so on.</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "534e1b91-2eaf-4923-af58-9bfd4daa9cf7",
   "metadata": {},
   "source": [
    "As we are going to deal with the huge amount of data, tradtional Database (RDBMS) are not the ideal choice. Big data can be handled in efficient way by using Hadoop file system. HDFS distributes the file automatically into manageable chunks if size 128 MB\n",
    "\n",
    "Steps\n",
    "1. To prepare data file for processing, that means storing the file on the Hadopp Distributed File System(HDFS). Hadoop HDFS is designed to handle large file and distribute it across cluster for processing. In our case we can upload the 1 TB sales trasaction file to HDFS.\n",
    "2. Data can be uploaded to HDFS\n",
    "    2.1 First we will create the directory on HDFS using hdfs ds -mkdir /SalesTransation\n",
    "    2.2 Upload file to hdfs using command hdfs dfs - put SalesTransaction.csv /SalesTransaction\n",
    "    3.2 HDFS will automatically \n",
    "3. Once data is uploaded HDFS will automatically manage the file in manageable chunks of 128 MB which will be easier to work with.\n",
    "4. Once data is stored in HDFS, the next step will be to write MapReduce program that will define how data should be read from the nodes. MapReduce diivides processins in two phases.\n",
    "    4.1 Map - Processes input sales trasactions and produce key value pairs. In our case key can be TransactionNo.\n",
    "    4.2 Reduce - Aggregates the data to produce the final output. Reduce will take the transactions from different partitions and aggregate them together.\n",
    "For easy reading of data i am going to use Hive that will make it easier to read and process data.\n",
    "\n",
    "For The steps will be following:\n",
    "\n",
    "        1. Upload input sales transaction file to HDFS under /examples/RetailSales\n",
    "        2. Create a SQL script, that will create a  Database name Sales and table name sales_transaction. Save the script in a .sql file.\n",
    "        3. upen cygwin and connect to hadoop bash\n",
    "        4. run the command hdfs -f \"Name of file containing script\"\n",
    "        5. Now connect to hive SQL \n",
    "        6. Run show databases; -> use sales; -> show tables; ->select * from sales_transactions LIMIT 2;\n",
    "        7. It will load the table on the command promt\n",
    "        8. Now go to URL \"http://localhost:50070/explorer.html#/user/hive/warehouse\" and check if the sales.db file exist under it.\n",
    "Please refer to section b for code for SQL script     \n",
    "once we are done we can not access the data from python and transform it in the required format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc7571-9316-42dd-ba44-5371ad0d2748",
   "metadata": {},
   "source": [
    "<h4>b. Write down a mapper and reducer in a language of your choice, so long as it is commented properly. What are the inputs to each function? What logic will each function perform? Clearly explain how the final answer is arrived at.</h4>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b68299d-cf47-4c0d-91bf-add4cae1d722",
   "metadata": {},
   "source": [
    "# Script will create database sales and table sales_transaction under it.\n",
    "create database sales;\n",
    "use sales;\n",
    "\n",
    "\n",
    "CREATE TABLE sales_transactions (\n",
    "  Store_ID FLOAT,\n",
    "  Transaction_Date STRING,\n",
    "  SKU1 STRING,\n",
    "  Quantity1 FLOAT,\n",
    "  UnitCost1 FLOAT,\n",
    "  SKU2 STRING,\n",
    "  Quantity2 FLOAT,\n",
    "  UnitCost2 FLOAT,\n",
    "  SKU3 STRING,\n",
    "  Quantity3 FLOAT,\n",
    "  UnitCost3 FLOAT\n",
    ")ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/examples/RetailSales';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5086aaaa-b574-476d-a6ef-9e31be8cc522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spark libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5992981a-dc04-4bc6-a1a4-85e5c0732b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all outputs in a block - not just the last one\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7564a962-d3d6-4d22-9c04-4e0bff01c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c035dbe90ced:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PythonSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1ed6ef2ce0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"PythonSpark\") \\\n",
    "        .config(\"hive.metastore.uris\",\n",
    "                \"thrift://hive-metastore:9083\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\",\n",
    "                \"http://namenode:50070/user/hive/warehouse\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(spark.version)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c8f1371-3034-4bc1-a90a-15148208d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#create structure for the data\n",
    "schema = StructType([\\\n",
    "    StructField(\"Store_ID\", StringType(), True),\\\n",
    "    StructField(\"Transaction_Date\", StringType(), True),\\\n",
    "    StructField(\"SKU1\",  StringType(), True),\n",
    "    StructField(\"Quantity1\",  StringType(), True),\n",
    "    StructField(\"UnitCost1\", StringType(), True),\n",
    "    StructField(\"SKU2\",  StringType(), True),\n",
    "    StructField(\"Quantity2\",  StringType(), True),\n",
    "    StructField(\"UnitCost2\", StringType(), True),\n",
    "    StructField(\"SKU3\",  StringType(), True),\n",
    "    StructField(\"Quantity3\",  StringType(), True),\n",
    "    StructField(\"UnitCost3\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4319725-f94e-41ce-bca5-928efb2374fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#readind data from the Retail Sales file on hive cluster\n",
    "sales_df = spark.read \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .schema(schema) \\\n",
    "                    .csv(\"hdfs://namenode:8020/examples/RetailSales\", sep=r\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f8f010b-4fff-4a3c-84f4-94974fce799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store_ID: string (nullable = true)\n",
      " |-- Transaction_Date: string (nullable = true)\n",
      " |-- SKU1: string (nullable = true)\n",
      " |-- Quantity1: string (nullable = true)\n",
      " |-- UnitCost1: string (nullable = true)\n",
      " |-- SKU2: string (nullable = true)\n",
      " |-- Quantity2: string (nullable = true)\n",
      " |-- UnitCost2: string (nullable = true)\n",
      " |-- SKU3: string (nullable = true)\n",
      " |-- Quantity3: string (nullable = true)\n",
      " |-- UnitCost3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2149c2e-1f2b-49a4-8a29-15652a848707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Store_ID='5', Transaction_Date='2/19/2023', SKU1='B398703', Quantity1='9', UnitCost1='161', SKU2='B617760', Quantity2='8', UnitCost2='473', SKU3='B252470', Quantity3='3', UnitCost3='132'),\n",
       " Row(Store_ID='8', Transaction_Date='11/30/2022', SKU1='B325734', Quantity1='2', UnitCost1='253', SKU2='B598408', Quantity2='1', UnitCost2='328', SKU3='B777874', Quantity3='9', UnitCost3='494'),\n",
       " Row(Store_ID='7', Transaction_Date='8/6/2022', SKU1='B914702', Quantity1='2', UnitCost1='178', SKU2='B441976', Quantity2='1', UnitCost2='111', SKU3='B785187', Quantity3='10', UnitCost3='292'),\n",
       " Row(Store_ID='2', Transaction_Date='10/6/2022', SKU1='B532921', Quantity1='2', UnitCost1='275', SKU2='B477851', Quantity2='2', UnitCost2='401', SKU3='B334946', Quantity3='9', UnitCost3='492'),\n",
       " Row(Store_ID='1', Transaction_Date='10/15/2022', SKU1='B554939', Quantity1='2', UnitCost1='182', SKU2='B913557', Quantity2='8', UnitCost2='345', SKU3='B483552', Quantity3='7', UnitCost3='331')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d2cdf11-d84a-4945-b448-01d327cbfb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df_p = spark.createDataFrame(sales_df.take(5)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cce6e72-90f7-4828-b81a-8713c721785e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store_ID</th>\n",
       "      <th>Transaction_Date</th>\n",
       "      <th>SKU1</th>\n",
       "      <th>Quantity1</th>\n",
       "      <th>UnitCost1</th>\n",
       "      <th>SKU2</th>\n",
       "      <th>Quantity2</th>\n",
       "      <th>UnitCost2</th>\n",
       "      <th>SKU3</th>\n",
       "      <th>Quantity3</th>\n",
       "      <th>UnitCost3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2/19/2023</td>\n",
       "      <td>B398703</td>\n",
       "      <td>9</td>\n",
       "      <td>161</td>\n",
       "      <td>B617760</td>\n",
       "      <td>8</td>\n",
       "      <td>473</td>\n",
       "      <td>B252470</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>11/30/2022</td>\n",
       "      <td>B325734</td>\n",
       "      <td>2</td>\n",
       "      <td>253</td>\n",
       "      <td>B598408</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "      <td>B777874</td>\n",
       "      <td>9</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8/6/2022</td>\n",
       "      <td>B914702</td>\n",
       "      <td>2</td>\n",
       "      <td>178</td>\n",
       "      <td>B441976</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>B785187</td>\n",
       "      <td>10</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>10/6/2022</td>\n",
       "      <td>B532921</td>\n",
       "      <td>2</td>\n",
       "      <td>275</td>\n",
       "      <td>B477851</td>\n",
       "      <td>2</td>\n",
       "      <td>401</td>\n",
       "      <td>B334946</td>\n",
       "      <td>9</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10/15/2022</td>\n",
       "      <td>B554939</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>B913557</td>\n",
       "      <td>8</td>\n",
       "      <td>345</td>\n",
       "      <td>B483552</td>\n",
       "      <td>7</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>8/25/2022</td>\n",
       "      <td>B400974</td>\n",
       "      <td>6</td>\n",
       "      <td>405</td>\n",
       "      <td>B150259</td>\n",
       "      <td>4</td>\n",
       "      <td>293</td>\n",
       "      <td>B872148</td>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2</td>\n",
       "      <td>8/7/2022</td>\n",
       "      <td>B708110</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>B137105</td>\n",
       "      <td>2</td>\n",
       "      <td>357</td>\n",
       "      <td>B406237</td>\n",
       "      <td>8</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>10</td>\n",
       "      <td>10/23/2022</td>\n",
       "      <td>B724754</td>\n",
       "      <td>2</td>\n",
       "      <td>318</td>\n",
       "      <td>B513070</td>\n",
       "      <td>5</td>\n",
       "      <td>327</td>\n",
       "      <td>B340658</td>\n",
       "      <td>7</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>2/22/2022</td>\n",
       "      <td>B919699</td>\n",
       "      <td>4</td>\n",
       "      <td>487</td>\n",
       "      <td>B357324</td>\n",
       "      <td>10</td>\n",
       "      <td>464</td>\n",
       "      <td>B912729</td>\n",
       "      <td>1</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>3</td>\n",
       "      <td>7/5/2022</td>\n",
       "      <td>B562895</td>\n",
       "      <td>4</td>\n",
       "      <td>134</td>\n",
       "      <td>B669837</td>\n",
       "      <td>10</td>\n",
       "      <td>316</td>\n",
       "      <td>B511058</td>\n",
       "      <td>3</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Store_ID Transaction_Date     SKU1 Quantity1 UnitCost1     SKU2 Quantity2  \\\n",
       "0          5        2/19/2023  B398703         9       161  B617760         8   \n",
       "1          8       11/30/2022  B325734         2       253  B598408         1   \n",
       "2          7         8/6/2022  B914702         2       178  B441976         1   \n",
       "3          2        10/6/2022  B532921         2       275  B477851         2   \n",
       "4          1       10/15/2022  B554939         2       182  B913557         8   \n",
       "..       ...              ...      ...       ...       ...      ...       ...   \n",
       "195        1        8/25/2022  B400974         6       405  B150259         4   \n",
       "196        2         8/7/2022  B708110         2       102  B137105         2   \n",
       "197       10       10/23/2022  B724754         2       318  B513070         5   \n",
       "198        1        2/22/2022  B919699         4       487  B357324        10   \n",
       "199        3         7/5/2022  B562895         4       134  B669837        10   \n",
       "\n",
       "    UnitCost2     SKU3 Quantity3 UnitCost3  \n",
       "0         473  B252470         3       132  \n",
       "1         328  B777874         9       494  \n",
       "2         111  B785187        10       292  \n",
       "3         401  B334946         9       492  \n",
       "4         345  B483552         7       331  \n",
       "..        ...      ...       ...       ...  \n",
       "195       293  B872148         3       101  \n",
       "196       357  B406237         8       104  \n",
       "197       327  B340658         7       109  \n",
       "198       464  B912729         1       309  \n",
       "199       316  B511058         3       362  \n",
       "\n",
       "[200 rows x 11 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00c3a0f6-e839-4dbe-a1e4-f436ac73805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Print transaction No:  0  is fraud\n",
      "\n",
      ">> Print transaction No:  1  is fraud\n",
      "\n",
      ">> Print transaction No:  2  is fraud\n",
      "\n",
      ">> Print transaction No:  3  is fraud\n",
      "\n",
      ">> Print transaction No:  4  is fraud\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#function to check the fraud transaction, will accept each row and share the output\n",
    "def isFraud(transaction) :\n",
    "    isfraud = 1\n",
    "    #print(transaction)\n",
    "    return(isfraud)\n",
    "\n",
    "#Loop will pass the dataset row by row to frad detection function and receive result in case transaction is identified as fraud. based on that the message is printed.\n",
    "for i in sales_df_p.itertuples(): \n",
    "    tran_isfraud = isFraud(i) \n",
    "    if tran_isfraud == 1 :\n",
    "        print('>> Print transaction No: ', str(i[0]) ,' is fraud\\n')\n",
    "    else:\n",
    "        print('Print transaction is Fraud')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf4824-a44f-4b69-bc5b-d25517e27108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
